# -*- coding: utf-8 -*-
"""Revisi20.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ZMJ27SXsyTJlCxxlDe9aaPzIaJodglw_
"""

pip install tfx

#import library
import tensorflow as tf
from tfx.components import CsvExampleGen, StatisticsGen, SchemaGen, ExampleValidator, Transform, Trainer, Tuner
from tfx.proto import example_gen_pb2
from tfx.orchestration.experimental.interactive.interactive_context import InteractiveContext
import os
import pandas as pd

PIPELINE_NAME = "sentimen-pipeline"
SCHEMA_PIPELINE_NAME = "sentimen-tfdv-schema"

PIPELINE_ROOT = os.path.join('pipelines', PIPELINE_NAME)
METADATA_PATH = os.path.join('metadata', PIPELINE_NAME, 'metadata.db')
SERVING_MODEL_DIR = os.path.join('serving_model', PIPELINE_NAME)

sentimen = pd.read_csv("data/dataset_komentar_instagram_cyberbullying.csv")
sentimen.info()

#data_path = "data"
#sentimen = sentimen.drop(["Id"], axis = 1)
#sentimen.to_csv(os.path.join(data_path, "sentimen.csv"), index = False)

import os

# Path data
data_path = "data"

# Drop kolom "Id" (jika ada)
sentimen = sentimen.drop(["Id"], axis=1)

# Konversi kolom "Sentiment" menjadi biner: negative = 0, positive = 1
sentimen["Sentiment"] = sentimen["Sentiment"].map({"negative": 0, "positive": 1})

# Simpan sebagai file CSV
sentimen.to_csv(os.path.join(data_path, "sentimen.csv"), index=False)

DATA_ROOT = 'data'

interactive_context = InteractiveContext(pipeline_root = PIPELINE_ROOT)

"""Data Ingestion"""

output = example_gen_pb2.Output(
    split_config = example_gen_pb2.SplitConfig(splits = [
        example_gen_pb2.SplitConfig.Split(name = "train", hash_buckets = 8),
        example_gen_pb2.SplitConfig.Split(name = "eval", hash_buckets = 2)
    ])
)

example_gen = CsvExampleGen(input_base = DATA_ROOT, output_config = output)

from  tfx.proto import example_gen_pb2

#Input has a single split 'input_dir/*
#Output 2 splits: train:eval=3:1.'
output = example_gen_pb2.Output(
            split_config=example_gen_pb2.SplitConfig(splits=[
                example_gen_pb2.SplitConfig.Split(name='train', hash_buckets=3),
                example_gen_pb2.SplitConfig.Split(name='eval', hash_buckets=1)
            ]))

example_gen = CsvExampleGen(input_base=DATA_ROOT, output_config=output)

import shutil

# Hapus folder .ipynb_checkpoints
checkpoints_path = os.path.join("data", ".ipynb_checkpoints")
if os.path.exists(checkpoints_path):
    shutil.rmtree(checkpoints_path)

print(os.listdir("data"))



example_gen = CsvExampleGen(input_base="data")
interactive_context.run(example_gen)

"""Data Validation
StatisticGen
"""

statistic_gen = StatisticsGen(
    examples = example_gen.outputs["examples"]
)

interactive_context.run(statistic_gen)

interactive_context.show(statistic_gen.outputs['statistics'])

schema_gen = SchemaGen(statistics = statistic_gen.outputs["statistics"])

interactive_context.run(schema_gen)

"""Membuat SchemaGen untuk memperoleh informasi"""

interactive_context.show(schema_gen.outputs["schema"])

example_validator = ExampleValidator(
    statistics = statistic_gen.outputs["statistics"],
    schema = schema_gen.outputs["schema"]
)

interactive_context.run(example_validator)

interactive_context.show(example_validator.outputs["anomalies"])

"""**Data Preprocessing**
Transform
"""

TRANSFORM_MODULE_FILE = "sentimen_transform.py"

"""TRANSFORM_MODULE_FILE = "sentimen_transform.py"
"""

# Commented out IPython magic to ensure Python compatibility.
# %%writefile {TRANSFORM_MODULE_FILE}
# 
# import tensorflow as tf
# 
# LABEL_KEY = "Sentiment"
# FEATURE_KEY = "Instagram Comment Text"
# 
# def transformed_name(key) :
#     return key + "_xf"
# 
# def preprocessing_fn(inputs) :
#     outputs = {}
#     outputs[transformed_name(LABEL_KEY)] = tf.cast(inputs[LABEL_KEY], tf.int64)
#     outputs[transformed_name(FEATURE_KEY)] = tf.strings.lower(inputs[FEATURE_KEY])
#     return outputs

# Definisikan schema untuk kolom Sentiment dan Instagram Comment Text
schema = {
    'Instagram Comment Text': tf.io.FixedLenFeature([], tf.string),
    'Sentiment': tf.io.FixedLenFeature([], tf.string),  # Jika Sentiment berupa teks
}

transform = Transform(
    examples = example_gen.outputs["examples"],
    schema = schema_gen.outputs['schema'],
    module_file = os.path.abspath(TRANSFORM_MODULE_FILE)
)

interactive_context.run(transform)

TRAINER_MODULE_FILE = "sentimen_trainer.py"

# Commented out IPython magic to ensure Python compatibility.
# %%writefile sentimen_trainer.py
# 
# import tensorflow as tf
# import tensorflow_transform as tft
# from tensorflow.keras import layers
# import os
# from tfx.components.trainer.fn_args_utils import FnArgs
# 
# # Definisi konstan
# LABEL_KEY = "Sentiment"
# FEATURE_KEY = "Instagram Comment Text"
# 
# # Fungsi untuk membuat nama fitur setelah transformasi
# def transformed_name(key):
#     return key + "_xf"
# 
# # Fungsi untuk membaca file TFRecord yang dikompresi
# def gzip_reader_fn(filenames):
#     return tf.data.TFRecordDataset(filenames, compression_type="GZIP")
# 
# # Fungsi untuk membuat input dataset
# def input_fn(file_pattern, tf_transform_output, num_epochs, batch_size=64):
#     transform_feature_spec = (
#         tf_transform_output.transformed_feature_spec().copy()
#     )
# 
#     dataset = tf.data.experimental.make_batched_features_dataset(
#         file_pattern=file_pattern,
#         batch_size=batch_size,
#         features=transform_feature_spec,
#         reader=gzip_reader_fn,
#         num_epochs=num_epochs,
#         label_key=transformed_name(LABEL_KEY),
#     )
#     return dataset
# 
# # Definisi TextVectorization
# VOCAB_SIZE = 10000
# SEQUENCE_LENGTH = 100
# 
# vectorize_layer = layers.TextVectorization(
#     standardize="lower_and_strip_punctuation",
#     max_tokens=VOCAB_SIZE,
#     output_mode='int',
#     output_sequence_length=SEQUENCE_LENGTH
# )
# 
# embedding_dim = 16
# 
# # Fungsi untuk membangun model
# def model_builder():
#     inputs = tf.keras.Input(
#         shape=(1,),
#         name=transformed_name(FEATURE_KEY),
#         dtype=tf.string
#     )
# 
#     reshaped_inputs = tf.reshape(inputs, [-1])
#     x = vectorize_layer(reshaped_inputs)
#     x = layers.Embedding(VOCAB_SIZE, embedding_dim, name="embedding")(x)
#     x = layers.GlobalAveragePooling1D()(x)
#     x = layers.Dense(64, activation="relu")(x)
#     x = layers.Dense(32, activation="relu")(x)
#     outputs = layers.Dense(1, activation="sigmoid")(x)
# 
#     model = tf.keras.Model(inputs=inputs, outputs=outputs)
# 
#     model.compile(
#         loss="binary_crossentropy",
#         optimizer=tf.keras.optimizers.Adam(0.01),
#         metrics=[tf.keras.metrics.BinaryAccuracy()]
#     )
# 
#     model.summary()
#     return model
# 
# # Fungsi untuk menyusun model untuk serving
# def _get_serve_tf_examples_fn(model, tf_transform_output):
#     model.tft_layer = tf_transform_output.transform_features_layer()
# 
#     @tf.function
#     def serve_tf_examples_fn(serialized_tf_examples):
#         feature_spec = tf_transform_output.raw_feature_spec()
#         feature_spec.pop(LABEL_KEY)
#         parsed_features = tf.io.parse_example(serialized_tf_examples, feature_spec)
#         transformed_features = model.tft_layer(parsed_features)
#         return model(transformed_features)
# 
#     return serve_tf_examples_fn
# 
# # Fungsi untuk menjalankan pipeline training
# def run_fn(fn_args: FnArgs):
#     log_dir = os.path.join(os.path.dirname(fn_args.serving_model_dir), 'logs')
# 
#     tensorboard_callback = tf.keras.callbacks.TensorBoard(
#         log_dir=log_dir, update_freq='batch'
#     )
# 
#     es = tf.keras.callbacks.EarlyStopping(
#         monitor='val_binary_accuracy',
#         mode='max',
#         verbose=1,
#         patience=10
#     )
# 
#     mc = tf.keras.callbacks.ModelCheckpoint(
#         fn_args.serving_model_dir,
#         monitor='val_binary_accuracy',
#         mode='max',
#         verbose=1,
#         save_best_only=True
#     )
# 
#     tf_transform_output = tft.TFTransformOutput(fn_args.transform_graph_path)
# 
#     train_set = input_fn(fn_args.train_files, tf_transform_output, num_epochs=10)
#     val_set = input_fn(fn_args.eval_files, tf_transform_output, num_epochs=10)
# 
#     # Adaptasi vectorize_layer pada fitur teks
#     vectorize_layer.adapt(
#         [x[0].numpy().decode('utf-8') for x in train_set.take(1)][0]
#     )
# 
#     model = model_builder()
# 
#     model.fit(
#         x=train_set,
#         validation_data=val_set,
#         callbacks=[tensorboard_callback, es, mc],
#         steps_per_epoch=1000,
#         validation_steps=1000,
#         epochs=10
#     )
# 
#     signatures = {
#         'serving_default':
#         _get_serve_tf_examples_fn(model, tf_transform_output).get_concrete_function(
#             tf.TensorSpec(
#                 shape=[None],
#                 dtype=tf.string,
#                 name='examples'
#             )
#         )
#     }
# 
# 
# 
#     #Menyimpan model pada serving_model_dir
#     model.save(fn_args.serving_model_dir, save_format = 'tf', signatures = signatures)
#

from tfx.proto import trainer_pb2

trainer = Trainer(
    module_file = os.path.abspath(TRAINER_MODULE_FILE),
    examples = transform.outputs['transformed_examples'],
    transform_graph = transform.outputs['transform_graph'],
    schema = schema_gen.outputs['schema'],
    train_args = trainer_pb2.TrainArgs(splits = ['train']),
    eval_args = trainer_pb2.EvalArgs(splits = ['eval'])
)

interactive_context.run(trainer)

from tfx.orchestration.metadata import Metadata

from tfx.types import artifact

import tensorflow as tf
import tensorflow_transform as tft
from tensorflow.keras import layers
import tensorflow_model_analysis as tfma
import tensorflow_data_validation as tfdv
from tfx.components import Trainer
from tfx.proto import trainer_pb2
from tfx.orchestration.metadata import Metadata
import os

# Define constants for your pipeline
LABEL_KEY = 'Sentiment_xf'
VOCAB_SIZE = 10000
embedding_dim = 128

# Input layer definition with unique name
def model_builder():
    inputs = tf.keras.Input(shape=(1,), name="Instagram_Comment_Text_xf", dtype=tf.string)
    reshaped_text = tf.reshape(inputs, [-1])
    vectorize_layer = tf.keras.layers.TextVectorization(max_tokens=VOCAB_SIZE, output_mode='int', output_sequence_length=200)
    x = vectorize_layer(reshaped_text)
    x = layers.Embedding(VOCAB_SIZE, embedding_dim, name="embedding")(x)
    x = layers.GlobalAveragePooling1D()(x)
    x = layers.Dense(64, activation="relu")(x)
    x = layers.Dense(32, activation="relu")(x)
    outputs = layers.Dense(1, activation="sigmoid")(x)

    model = tf.keras.Model(inputs=inputs, outputs=outputs)
    model.compile(
        loss="binary_crossentropy",
        optimizer=tf.keras.optimizers.Adam(0.01),
        metrics=[tf.keras.metrics.BinaryAccuracy()],
    )
    return model

# Define function to serve transformed examples
def _get_serve_tf_examples_fn(model, tf_transform_output):
    model.tft_layer = tf_transform_output.transform_features_layer()

    @tf.function
    def serve_tf_examples_fn(serialized_tf_examples):
        # Parse and transform features
        feature_spec = tf_transform_output.raw_feature_spec()
        feature_spec.pop(LABEL_KEY)  # Remove label from feature spec
        parsed_features = tf.io.parse_example(serialized_tf_examples, feature_spec)
        transformed_features = model.tft_layer(parsed_features)
        return model(transformed_features)

    return serve_tf_examples_fn

# TensorFlow Transform function
def preprocessing_fn(inputs):
    """Preprocess the data."""
    outputs = {}
    # Assume `Instagram Comment Text_xf` is your text column
    outputs['Instagram_Comment_Text_xf'] = tft.compute_and_apply_vocabulary(inputs['Instagram_Comment_Text_xf'])
    outputs['Sentiment_xf'] = inputs['Sentiment_xf']
    return outputs

# Define TFX trainer component
def trainer_fn():
    # Load the transformed data
    tf_transform_output = tft.TFTransformOutput(TRANSFORMER_OUTPUT)

    # Build the model
    model = model_builder()

    # Specify inputs and outputs for model
    inputs = {"Instagram_Comment_Text_xf": tf.keras.Input(shape=(1,), dtype=tf.string, name="Instagram_Comment_Text_xf")}
    outputs = model(inputs)

    # Create the serving function for transformed examples
    serve_fn = _get_serve_tf_examples_fn(model, tf_transform_output)

    # Train the model
    model.fit(train_data, epochs=5, batch_size=32)

    # Save the model and serving function
    model.save(MODEL_DIR)
    return model

# Define TFX pipeline configuration
def create_pipeline():
    # Define the trainer component
    trainer = Trainer(
        module_file=trainer_fn,
        custom_executor_spec=None,  # Optional executor spec if you need custom
        schema=schema,  # Define your schema here
        transform_output=tf_transform_output,
        output=StandardArtifacts.Model(),  # Define the output artifact for the trained model
    )
    return trainer

# Example of running the pipeline
interactive_context.run(create_pipeline())

from tfx.dsl.components.common.resolver import Resolver
from tfx.dsl.input_resolution.strategies.latest_blessed_model_strategy import LatestBlessedModelStrategy
from tfx.types import Channel
from tfx.types.standard_artifacts import Model, ModelBlessing

model_resolver = Resolver(
    strategy_class = LatestBlessedModelStrategy,
    model = Channel(type = Model),
    model_blessing = Channel(type = ModelBlessing)
).with_id('Latest_blessed_model_resolver')

interactive_context.run(model_resolver)

import tensorflow_model_analysis as tfma

eval_config = tfma.EvalConfig(
    model_specs = [tfma.ModelSpec(label_key = 'label')],
    slicing_specs = [tfma.SlicingSpec()],
    metrics_specs = [
        tfma.MetricsSpec(metrics = [
                tfma.MetricConfig(class_name = 'ExampleCount'),
                tfma.MetricConfig(class_name = 'AUC'),
                tfma.MetricConfig(class_name = 'FalsePositives'),
                tfma.MetricConfig(class_name = 'TruePositives'),
                tfma.MetricConfig(class_name = 'FalseNegatives'),
                tfma.MetricConfig(class_name = 'TrueNegatives'),
                tfma.MetricConfig(class_name = 'BinaryAccuracy',
                    threshold = tfma.MetricThreshold(
                        value_threshold = tfma.GenericValueThreshold(
                            lower_bound = {'value' : 0.5}
                    ),
                    change_threshold = tfma.GenericChangeThreshold(
                        direction = tfma.MetricDirection.HIGHER_IS_BETTER,
                            absolute = {'value' : 0.0001}
                    )
                )
            )
        ])
    ]
)

from tfx.components import Evaluator

evaluator = Evaluator(
    examples = example_gen.outputs['examples'],
    model = trainer.outputs['model'],
    baseline_model = model_resolver.outputs['model'],
    eval_config = eval_config
)

interactive_context.run(evaluator)

eval_result = evaluator.outputs['evaluation'].get()[0].uri
tfma_result = tfma.load_eval_result(eval_result)
tfma.view.render_slicing_metrics(tfma_result)
tfma.addons.fairness.view.widget_view.render_fairness_indicator(
    tfma_result
)

from tfx.components import Pusher
from tfx.proto import pusher_pb2

pusher = Pusher(
    model = trainer.outputs['model'],
    model_blessing = evaluator.outputs['blessing'],
    push_destination = pusher_pb2.PushDestination(
        filesystem = pusher_pb2.PushDestination.Filesystem(
            base_directory = "serving_model_dir/stress-prediction-model"
        )
    )
)

interactive_context.run(pusher)